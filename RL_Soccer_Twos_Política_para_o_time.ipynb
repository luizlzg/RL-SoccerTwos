{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install"
      ],
      "metadata": {
        "id": "qajgZWbmizIn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMkM8zaJCDWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b9dd48-be7a-4cba-c470-bd95a77fb496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting soccer-twos\n",
            "  Downloading soccer_twos-0.1.14-py3-none-any.whl (20 kB)\n",
            "Collecting mlagents-envs==0.27.0\n",
            "  Downloading mlagents_envs-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym==0.19.0\n",
            "  Downloading gym-0.19.0.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym-unity==0.27.0\n",
            "  Downloading gym_unity-0.27.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting mlagents==0.27.0\n",
            "  Downloading mlagents-0.27.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.19.0->soccer-twos) (1.22.4)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0->soccer-twos) (2.11.2)\n",
            "Requirement already satisfied: Pillow>=4.2.1 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0->soccer-twos) (8.4.0)\n",
            "Requirement already satisfied: pyyaml>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0->soccer-twos) (6.0)\n",
            "Collecting cattrs<1.7,>=1.1.0\n",
            "  Downloading cattrs-1.5.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0->soccer-twos) (3.1.0)\n",
            "Collecting torch<1.9.0,>=1.6.0\n",
            "  Downloading torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.1/804.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0->soccer-twos) (22.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0->soccer-twos) (3.19.6)\n",
            "Requirement already satisfied: grpcio>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0->soccer-twos) (1.51.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (3.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (1.4.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (2.16.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.9.0,>=1.6.0->mlagents==0.27.0->soccer-twos) (4.5.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (3.14.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->mlagents==0.27.0->soccer-twos) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.19.0-py3-none-any.whl size=1663114 sha256=9a5097d3a99845eacecc6f7f14a68330a41e114ae826e586b06b250bbf371260\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/36/28/628f4dd3779e4037a6fca1aaed76827ffa4315c3ab6bfadcf6\n",
            "Successfully built gym\n",
            "Installing collected packages: torch, cloudpickle, cattrs, mlagents-envs, gym, gym-unity, mlagents, soccer-twos\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cattrs-1.5.0 cloudpickle-1.6.0 gym-0.19.0 gym-unity-0.27.0 mlagents-0.27.0 mlagents-envs-0.27.0 soccer-twos-0.1.14 torch-1.8.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray==2.2\n",
            "  Downloading ray-2.2.0-cp38-cp38-manylinux2014_x86_64.whl (57.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (1.3.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (3.19.6)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (22.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (1.0.4)\n",
            "Collecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.19.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (1.51.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (3.9.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from ray==2.2) (2.25.1)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.24->ray==2.2) (3.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray==2.2) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray==2.2) (5.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->ray==2.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->ray==2.2) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->ray==2.2) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->ray==2.2) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray==2.2) (3.14.0)\n",
            "Installing collected packages: distlib, virtualenv, ray\n",
            "Successfully installed distlib-0.3.6 ray-2.2.0 virtualenv-20.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install soccer-twos\n",
        "!pip install ray==2.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.19.0\n",
        "!pip install gym-unity==0.27.0\n",
        "!pip install mlagents==0.27.0\n",
        "!pip install mlagents-envs==0.27.0"
      ],
      "metadata": {
        "id": "NUy9zC4eJTOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f50bfc-7b6d-4fbd-db3c-59160dc53e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.19.0 in /usr/local/lib/python3.8/dist-packages (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.19.0) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.19.0) (1.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym-unity==0.27.0 in /usr/local/lib/python3.8/dist-packages (0.27.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from gym-unity==0.27.0) (0.19.0)\n",
            "Requirement already satisfied: mlagents-envs==0.27.0 in /usr/local/lib/python3.8/dist-packages (from gym-unity==0.27.0) (0.27.0)\n",
            "Requirement already satisfied: Pillow>=4.2.1 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0->gym-unity==0.27.0) (8.4.0)\n",
            "Requirement already satisfied: grpcio>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0->gym-unity==0.27.0) (1.51.3)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0->gym-unity==0.27.0) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0->gym-unity==0.27.0) (6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0->gym-unity==0.27.0) (1.6.0)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0->gym-unity==0.27.0) (3.19.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mlagents==0.27.0 in /usr/local/lib/python3.8/dist-packages (0.27.0)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (2.11.2)\n",
            "Requirement already satisfied: mlagents-envs==0.27.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (0.27.0)\n",
            "Requirement already satisfied: grpcio>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (1.51.3)\n",
            "Requirement already satisfied: pyyaml>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (6.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (3.1.0)\n",
            "Requirement already satisfied: cattrs<1.7,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (1.5.0)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (22.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (1.22.4)\n",
            "Requirement already satisfied: Pillow>=4.2.1 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (8.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (3.19.6)\n",
            "Requirement already satisfied: torch<1.9.0,>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from mlagents==0.27.0) (1.8.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0->mlagents==0.27.0) (1.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (2.25.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (2.16.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=1.15->mlagents==0.27.0) (0.4.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.9.0,>=1.6.0->mlagents==0.27.0) (4.5.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0) (5.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->mlagents==0.27.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->mlagents==0.27.0) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents==0.27.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents==0.27.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents==0.27.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents==0.27.0) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->mlagents==0.27.0) (3.14.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->mlagents==0.27.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->mlagents==0.27.0) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mlagents-envs==0.27.0 in /usr/local/lib/python3.8/dist-packages (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0) (1.22.4)\n",
            "Requirement already satisfied: grpcio>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0) (1.51.3)\n",
            "Requirement already satisfied: pyyaml>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0) (6.0)\n",
            "Requirement already satisfied: Pillow>=4.2.1 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0) (8.4.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0) (1.6.0)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.8/dist-packages (from mlagents-envs==0.27.0) (3.19.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports e utils"
      ],
      "metadata": {
        "id": "NdIH7t5j3qT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "import ray\n",
        "from ray import tune\n",
        "from soccer_twos import EnvType\n",
        "import numpy as np\n",
        "import os\n",
        "from ray.rllib import MultiAgentEnv\n",
        "from ray.rllib.agents.ppo import ppo\n",
        "from ray.tune.logger import pretty_print\n",
        "import soccer_twos\n",
        "import collections\n",
        "import random\n",
        "from collections import deque\n",
        "from gym_unity.envs import ActionFlattener\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "DZSmdq4FF8i9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb30f05-c389-4abc-e2f3-cd1da4ad244a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ray.rllib.utils.compression:lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
        "    \"\"\"\n",
        "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
        "    \"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "def create_rllib_env(env_config: dict = {}):\n",
        "    \"\"\"\n",
        "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
        "    Args:\n",
        "        env_config: configuration for the environment.\n",
        "            You may specify the following keys:\n",
        "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
        "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
        "    \"\"\"\n",
        "    if hasattr(env_config, \"worker_index\"):\n",
        "        env_config[\"worker_id\"] = (\n",
        "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
        "            + env_config.vector_index\n",
        "        )\n",
        "    env = soccer_twos.make(**env_config)\n",
        "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
        "        # is multiagent by default, is only disabled if explicitly set to False\n",
        "        return env\n",
        "    return RLLibWrapper(env)"
      ],
      "metadata": {
        "id": "Quy0s4SQF_v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune.registry.register_env(\"Soccer\", create_rllib_env)\n",
        "temp_env = create_rllib_env({\"variation\": EnvType.multiagent_player})\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "temp_env.close()"
      ],
      "metadata": {
        "id": "gl88Ev5FGBVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d09e54-1b2f-401b-9b4f-23764c044cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Connected new brain: SoccerTwos?team=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Connected new brain: SoccerTwos?team=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_policy(id):\n",
        "  if id == 0 or id == 1:\n",
        "    return f\"policy_0\"\n",
        "  else:\n",
        "    return f\"policy_1\""
      ],
      "metadata": {
        "id": "naaUB0P0bBfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento"
      ],
      "metadata": {
        "id": "Zfk4YfnEi5F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "environment_id = \"Soccer\"\n",
        "\n",
        "config = {\n",
        "        \"num_gpus\": 1,\n",
        "        \"num_workers\":2,\n",
        "        \"num_rollout_workers\":1,\n",
        "        \"num_envs_per_worker\" : 1,\n",
        "        \"rollout_fragment_length\" : 1,\n",
        "        \"framework\": \"torch\",\n",
        "        \"ignore_worker_failures\": True,\n",
        "        \"train_batch_size\": 256,\n",
        "        \"lr\": 1e-3,\n",
        "        \"lambda\": .95,\n",
        "        \"gamma\": .99,\n",
        "        \"entropy_coeff\": 0.01,\n",
        "        \"kl_coeff\": 1.0,\n",
        "        \"clip_param\": 0.2,\n",
        "        \"num_sgd_iter\": 10,\n",
        "        \"vf_loss_coeff\": 1e-4,\n",
        "        \"vf_clip_param\": 100000.0,\n",
        "        \"multiagent\": {\n",
        "            \"policies\": {\n",
        "                \"policy_0\": (None, obs_space, act_space, {}),\n",
        "                \"policy_1\": (None, obs_space, act_space, {})\n",
        "            },\n",
        "            \"policy_mapping_fn\": agent_policy,\n",
        "        },\n",
        "        \"env\": \"Soccer\",\n",
        "        \"env_config\": {\n",
        "            \"num_envs_per_worker\": 1,\n",
        "            \"variation\": EnvType.multiagent_player,\n",
        "        },\n",
        "        \"model\": {\n",
        "        \"fcnet_hiddens\": [512, 256],\n",
        "        \"fcnet_activation\": \"tanh\",\n",
        "      },\n",
        "    }\n",
        "ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
        "ppo_config.update(config)"
      ],
      "metadata": {
        "id": "LcktMkUXfOLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "with open('/content/results2/ppo/algorithm_state.pkl', \"rb\") as f:\n",
        "          checkpoint = pickle.load(f)"
      ],
      "metadata": {
        "id": "hpEtfl1bhBVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ppo.PPOTrainer(config=checkpoint['config'])"
      ],
      "metadata": {
        "id": "8W1RinC5GJQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3bcbf39-a846-469d-dd16-b351d40a8469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=40946)\u001b[0m 2023-02-28 02:51:22,956\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=40946)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=40946)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=40946)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=40946)\u001b[0m 2023-02-28 02:51:24,398\tWARNING env.py:247 -- Your MultiAgentEnv <RLLibWrapper<EnvChannelWrapper<MultiAgentUnityWrapper instance>>> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(RolloutWorker pid=40946)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=40946)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=40946)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-02-28 02:51:24,617\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ray.shutdown()"
      ],
      "metadata": {
        "id": "8fpqyiIpIsbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = './results/ppo'"
      ],
      "metadata": {
        "id": "yOhSmguPIcjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = []\n",
        "for i in range(int(1e3)): # train iter\n",
        "        r.append(trainer.train())\n",
        "        print(pretty_print(r[i]))\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            if not os.path.exists(CHECKPOINT_PATH):\n",
        "                os.makedirs(CHECKPOINT_PATH)\n",
        "            trainer.save_checkpoint(CHECKPOINT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw_PmgyhcR3o",
        "outputId": "b071428f-24ef-4de8-ad85-058076c66a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(RolloutWorker pid=40946)\u001b[0m 2023-02-28 02:51:41,732\tWARNING deprecation.py:47 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386]\n",
            "    policy_policy_1_reward: [0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482, 0.24879999458789825,\n",
            "      0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509,\n",
            "      0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.5685999989509583, 0.5685999989509583, 0.7770000100135803, 0.7770000100135803,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845, 0.2896000146865845,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9544000029563904\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.05458199987187982\n",
            "    policy_1: -0.09810199916362762\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.150560451472029\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.307025746811625\n",
            "    mean_inference_ms: 3.9612041038781793\n",
            "    mean_raw_obs_processing_ms: 6.878464823075729\n",
            "time_since_restore: 5623.584074020386\n",
            "time_this_iter_s: 4.875703811645508\n",
            "time_total_s: 5623.584074020386\n",
            "timers:\n",
            "  learn_throughput: 259.472\n",
            "  learn_time_ms: 986.62\n",
            "  synch_weights_time_ms: 6.246\n",
            "  training_iteration_time_ms: 5563.701\n",
            "timestamp: 1677558381\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 249344\n",
            "training_iteration: 974\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 998400\n",
            "counters:\n",
            "  num_agent_steps_sampled: 998400\n",
            "  num_agent_steps_trained: 998400\n",
            "  num_env_steps_sampled: 249600\n",
            "  num_env_steps_trained: 249600\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-26-28\n",
            "done: false\n",
            "episode_len_mean: 852.76\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3053679980710149\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 306\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 0.7216268945485353\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.9296001315116884\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.11183160841465\n",
            "        kl: 0.021381121892035226\n",
            "        policy_loss: -0.11170890792272985\n",
            "        total_loss: -0.12557569360360504\n",
            "        vf_explained_var: 0.6774457186460495\n",
            "        vf_loss: 0.0002834427163179498\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 38980.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.640246403217316\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.2071798101067543\n",
            "        kl: 0.016551081656936993\n",
            "        policy_loss: -0.11350669278763234\n",
            "        total_loss: -0.11867582257837057\n",
            "        vf_explained_var: 0.7556608110666275\n",
            "        vf_loss: 0.0008506412748829461\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 38980.5\n",
            "  num_agent_steps_sampled: 998400\n",
            "  num_agent_steps_trained: 998400\n",
            "  num_env_steps_sampled: 249600\n",
            "  num_env_steps_trained: 249600\n",
            "iterations_since_restore: 975\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 998400\n",
            "num_agent_steps_trained: 998400\n",
            "num_env_steps_sampled: 249600\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 249600\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 86.57777777777778\n",
            "  ram_util_percent: 37.81111111111111\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9544000029563904\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.05458199987187982\n",
            "  policy_1: -0.09810199916362762\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.150560451472029\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.307025746811625\n",
            "  mean_inference_ms: 3.9612041038781793\n",
            "  mean_raw_obs_processing_ms: 6.878464823075729\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 852.76\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3053679980710149\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 46, 375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000,\n",
            "      1000, 1000, 1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000,\n",
            "      107, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000,\n",
            "      1000, 711, 1000, 1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000,\n",
            "      1000, 962, 899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000,\n",
            "      982, 1000, 1000, 1000, 1000, 530]\n",
            "    episode_reward: [0.0, -0.09119999408721924, -0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018,\n",
            "      0.0, -0.618399977684021, 0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036,\n",
            "      -1.5024000108242035, 0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012,\n",
            "      0.0, -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.420799970626831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0,\n",
            "      -0.7208000421524048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799,\n",
            "      0.0, -1.3191999793052673, 0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675,\n",
            "      0.0, 0.0, -1.9231999963521957, -1.7971999943256378, -0.09640002250671387, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.3087999820709229, -1.9472000002861023, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8955999612808228, -1.237999975681305,\n",
            "      0.0, 0.0, 0.0, -1.9635999985039234, 0.0, 0.0, 0.0, 0.0, -1.0587999820709229]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.9544000029563904, 0.9544000029563904, 0.625,\n",
            "      0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386]\n",
            "    policy_policy_1_reward: [0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482, 0.24879999458789825,\n",
            "      0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509,\n",
            "      0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.5685999989509583, 0.5685999989509583, 0.7770000100135803, 0.7770000100135803,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845, 0.2896000146865845,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9544000029563904\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.05458199987187982\n",
            "    policy_1: -0.09810199916362762\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.150560451472029\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.307025746811625\n",
            "    mean_inference_ms: 3.9612041038781793\n",
            "    mean_raw_obs_processing_ms: 6.878464823075729\n",
            "time_since_restore: 5629.856921672821\n",
            "time_this_iter_s: 6.272847652435303\n",
            "time_total_s: 5629.856921672821\n",
            "timers:\n",
            "  learn_throughput: 250.0\n",
            "  learn_time_ms: 1024.0\n",
            "  synch_weights_time_ms: 6.188\n",
            "  training_iteration_time_ms: 5707.965\n",
            "timestamp: 1677558388\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 249600\n",
            "training_iteration: 975\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 999424\n",
            "counters:\n",
            "  num_agent_steps_sampled: 999424\n",
            "  num_agent_steps_trained: 999424\n",
            "  num_env_steps_sampled: 249856\n",
            "  num_env_steps_trained: 249856\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-26-33\n",
            "done: false\n",
            "episode_len_mean: 852.76\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3053679980710149\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 306\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.905182844400406\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.0864313833415509\n",
            "        kl: 0.016962579819977307\n",
            "        policy_loss: -0.10980521086603404\n",
            "        total_loss: -0.12049604258500039\n",
            "        vf_explained_var: 0.52798131108284\n",
            "        vf_loss: 0.00021193491757003357\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39020.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.713562750816345\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.5568846076726914\n",
            "        kl: 0.0184351741092156\n",
            "        policy_loss: -0.10699626991990954\n",
            "        total_loss: -0.11048150761052966\n",
            "        vf_explained_var: 0.6496780201792717\n",
            "        vf_loss: 0.0004962347440596205\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39020.5\n",
            "  num_agent_steps_sampled: 999424\n",
            "  num_agent_steps_trained: 999424\n",
            "  num_env_steps_sampled: 249856\n",
            "  num_env_steps_trained: 249856\n",
            "iterations_since_restore: 976\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 999424\n",
            "num_agent_steps_trained: 999424\n",
            "num_env_steps_sampled: 249856\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 249856\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 74.35555555555555\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9544000029563904\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.05458199987187982\n",
            "  policy_1: -0.09810199916362762\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.150560451472029\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.307025746811625\n",
            "  mean_inference_ms: 3.9612041038781793\n",
            "  mean_raw_obs_processing_ms: 6.878464823075729\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 852.76\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3053679980710149\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 46, 375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000,\n",
            "      1000, 1000, 1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000,\n",
            "      107, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000,\n",
            "      1000, 711, 1000, 1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000,\n",
            "      1000, 962, 899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000,\n",
            "      982, 1000, 1000, 1000, 1000, 530]\n",
            "    episode_reward: [0.0, -0.09119999408721924, -0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018,\n",
            "      0.0, -0.618399977684021, 0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036,\n",
            "      -1.5024000108242035, 0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012,\n",
            "      0.0, -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.420799970626831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0,\n",
            "      -0.7208000421524048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799,\n",
            "      0.0, -1.3191999793052673, 0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675,\n",
            "      0.0, 0.0, -1.9231999963521957, -1.7971999943256378, -0.09640002250671387, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.3087999820709229, -1.9472000002861023, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8955999612808228, -1.237999975681305,\n",
            "      0.0, 0.0, 0.0, -1.9635999985039234, 0.0, 0.0, 0.0, 0.0, -1.0587999820709229]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.9544000029563904, 0.9544000029563904, 0.625,\n",
            "      0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386]\n",
            "    policy_policy_1_reward: [0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482, 0.24879999458789825,\n",
            "      0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509,\n",
            "      0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.5685999989509583, 0.5685999989509583, 0.7770000100135803, 0.7770000100135803,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845, 0.2896000146865845,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9544000029563904\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.05458199987187982\n",
            "    policy_1: -0.09810199916362762\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.150560451472029\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.307025746811625\n",
            "    mean_inference_ms: 3.9612041038781793\n",
            "    mean_raw_obs_processing_ms: 6.878464823075729\n",
            "time_since_restore: 5635.676506996155\n",
            "time_this_iter_s: 5.81958532333374\n",
            "time_total_s: 5635.676506996155\n",
            "timers:\n",
            "  learn_throughput: 262.639\n",
            "  learn_time_ms: 974.722\n",
            "  synch_weights_time_ms: 5.808\n",
            "  training_iteration_time_ms: 5611.614\n",
            "timestamp: 1677558393\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 249856\n",
            "training_iteration: 976\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1000448\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1000448\n",
            "  num_agent_steps_trained: 1000448\n",
            "  num_env_steps_sampled: 250112\n",
            "  num_env_steps_trained: 250112\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-26-38\n",
            "done: false\n",
            "episode_len_mean: 852.76\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3053679980710149\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 306\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.8396624088287354\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3331861466169357\n",
            "        kl: 0.018561722241961898\n",
            "        policy_loss: -0.10824205912649632\n",
            "        total_loss: -0.1165467199869454\n",
            "        vf_explained_var: 0.6096423774957657\n",
            "        vf_loss: 0.00011490816577861552\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39060.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.716714471578598\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3429026767611503\n",
            "        kl: 0.016027004716965408\n",
            "        policy_loss: -0.10441082809120417\n",
            "        total_loss: -0.11101701776497067\n",
            "        vf_explained_var: 0.6968357771635055\n",
            "        vf_loss: 0.0004009490101452684\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39060.5\n",
            "  num_agent_steps_sampled: 1000448\n",
            "  num_agent_steps_trained: 1000448\n",
            "  num_env_steps_sampled: 250112\n",
            "  num_env_steps_trained: 250112\n",
            "iterations_since_restore: 977\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1000448\n",
            "num_agent_steps_trained: 1000448\n",
            "num_env_steps_sampled: 250112\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 250112\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 54.62857142857143\n",
            "  ram_util_percent: 37.800000000000004\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9544000029563904\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.05458199987187982\n",
            "  policy_1: -0.09810199916362762\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.150560451472029\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.307025746811625\n",
            "  mean_inference_ms: 3.9612041038781793\n",
            "  mean_raw_obs_processing_ms: 6.878464823075729\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 852.76\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3053679980710149\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 46, 375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000,\n",
            "      1000, 1000, 1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000,\n",
            "      107, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000,\n",
            "      1000, 711, 1000, 1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000,\n",
            "      1000, 962, 899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000,\n",
            "      982, 1000, 1000, 1000, 1000, 530]\n",
            "    episode_reward: [0.0, -0.09119999408721924, -0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018,\n",
            "      0.0, -0.618399977684021, 0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036,\n",
            "      -1.5024000108242035, 0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012,\n",
            "      0.0, -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.420799970626831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0,\n",
            "      -0.7208000421524048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799,\n",
            "      0.0, -1.3191999793052673, 0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675,\n",
            "      0.0, 0.0, -1.9231999963521957, -1.7971999943256378, -0.09640002250671387, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.3087999820709229, -1.9472000002861023, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8955999612808228, -1.237999975681305,\n",
            "      0.0, 0.0, 0.0, -1.9635999985039234, 0.0, 0.0, 0.0, 0.0, -1.0587999820709229]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.9544000029563904, 0.9544000029563904, 0.625,\n",
            "      0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386]\n",
            "    policy_policy_1_reward: [0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482, 0.24879999458789825,\n",
            "      0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509,\n",
            "      0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.5685999989509583, 0.5685999989509583, 0.7770000100135803, 0.7770000100135803,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845, 0.2896000146865845,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9544000029563904\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.05458199987187982\n",
            "    policy_1: -0.09810199916362762\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.150560451472029\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.307025746811625\n",
            "    mean_inference_ms: 3.9612041038781793\n",
            "    mean_raw_obs_processing_ms: 6.878464823075729\n",
            "time_since_restore: 5640.505949497223\n",
            "time_this_iter_s: 4.829442501068115\n",
            "time_total_s: 5640.505949497223\n",
            "timers:\n",
            "  learn_throughput: 262.078\n",
            "  learn_time_ms: 976.807\n",
            "  synch_weights_time_ms: 5.852\n",
            "  training_iteration_time_ms: 5560.605\n",
            "timestamp: 1677558398\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 250112\n",
            "training_iteration: 977\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1001472\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1001472\n",
            "  num_agent_steps_trained: 1001472\n",
            "  num_env_steps_sampled: 250368\n",
            "  num_env_steps_trained: 250368\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-26-44\n",
            "done: false\n",
            "episode_len_mean: 852.76\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3053679980710149\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 1\n",
            "episodes_total: 307\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.8832046389579773\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1975996695458888\n",
            "        kl: 0.016691883704709552\n",
            "        policy_loss: -0.1150739565724507\n",
            "        total_loss: -0.1258380349201616\n",
            "        vf_explained_var: 0.5892286613583565\n",
            "        vf_loss: 9.145599606199539e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39100.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.7365030467510225\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1934708036482333\n",
            "        kl: 0.018021142762582775\n",
            "        policy_loss: -0.11113816571887583\n",
            "        total_loss: -0.11538398409029468\n",
            "        vf_explained_var: 0.7007761716842651\n",
            "        vf_loss: 0.00029738663288298995\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39100.5\n",
            "  num_agent_steps_sampled: 1001472\n",
            "  num_agent_steps_trained: 1001472\n",
            "  num_env_steps_sampled: 250368\n",
            "  num_env_steps_trained: 250368\n",
            "iterations_since_restore: 978\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1001472\n",
            "num_agent_steps_trained: 1001472\n",
            "num_env_steps_sampled: 250368\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 250368\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 88.9\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9544000029563904\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.05458199987187982\n",
            "  policy_1: -0.09810199916362762\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15055429469556864\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306935556404629\n",
            "  mean_inference_ms: 3.961051190951073\n",
            "  mean_raw_obs_processing_ms: 6.87827168104954\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 852.76\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3053679980710149\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 1\n",
            "  hist_stats:\n",
            "    episode_lengths: [46, 375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000,\n",
            "      1000, 1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000,\n",
            "      711, 1000, 1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000,\n",
            "      962, 899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000,\n",
            "      1000, 1000, 1000, 530, 1000]\n",
            "    episode_reward: [-0.09119999408721924, -0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018,\n",
            "      0.0, -0.618399977684021, 0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036,\n",
            "      -1.5024000108242035, 0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012,\n",
            "      0.0, -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.420799970626831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0,\n",
            "      -0.7208000421524048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799,\n",
            "      0.0, -1.3191999793052673, 0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675,\n",
            "      0.0, 0.0, -1.9231999963521957, -1.7971999943256378, -0.09640002250671387, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.3087999820709229, -1.9472000002861023, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8955999612808228, -1.237999975681305,\n",
            "      0.0, 0.0, 0.0, -1.9635999985039234, 0.0, 0.0, 0.0, 0.0, -1.0587999820709229,\n",
            "      0.0]\n",
            "    policy_policy_0_reward: [0.9544000029563904, 0.9544000029563904, 0.625, 0.625,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [-1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482, 0.24879999458789825,\n",
            "      0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509,\n",
            "      0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.5685999989509583, 0.5685999989509583, 0.7770000100135803, 0.7770000100135803,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845, 0.2896000146865845,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9544000029563904\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.05458199987187982\n",
            "    policy_1: -0.09810199916362762\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15055429469556864\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306935556404629\n",
            "    mean_inference_ms: 3.961051190951073\n",
            "    mean_raw_obs_processing_ms: 6.87827168104954\n",
            "time_since_restore: 5646.618212461472\n",
            "time_this_iter_s: 6.112262964248657\n",
            "time_total_s: 5646.618212461472\n",
            "timers:\n",
            "  learn_throughput: 253.732\n",
            "  learn_time_ms: 1008.938\n",
            "  synch_weights_time_ms: 5.957\n",
            "  training_iteration_time_ms: 5695.047\n",
            "timestamp: 1677558404\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 250368\n",
            "training_iteration: 978\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1002496\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1002496\n",
            "  num_agent_steps_trained: 1002496\n",
            "  num_env_steps_sampled: 250624\n",
            "  num_env_steps_trained: 250624\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-26-51\n",
            "done: false\n",
            "episode_len_mean: 852.76\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3053679980710149\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 307\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.8336822986602783\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.2223922245204448\n",
            "        kl: 0.018381114327005666\n",
            "        policy_loss: -0.11437046069186181\n",
            "        total_loss: -0.12281082046683878\n",
            "        vf_explained_var: 0.7893031746149063\n",
            "        vf_loss: 5.515678149095038e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39140.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.6830838084220887\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.4845975413918495\n",
            "        kl: 0.017548531040495842\n",
            "        policy_loss: -0.10945868631824851\n",
            "        total_loss: -0.11377662825398147\n",
            "        vf_explained_var: 0.627567395567894\n",
            "        vf_loss: 0.0002148642211977858\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39140.5\n",
            "  num_agent_steps_sampled: 1002496\n",
            "  num_agent_steps_trained: 1002496\n",
            "  num_env_steps_sampled: 250624\n",
            "  num_env_steps_trained: 250624\n",
            "iterations_since_restore: 979\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1002496\n",
            "num_agent_steps_trained: 1002496\n",
            "num_env_steps_sampled: 250624\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 250624\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 77.15555555555555\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9544000029563904\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.05458199987187982\n",
            "  policy_1: -0.09810199916362762\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15055429469556864\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306935556404629\n",
            "  mean_inference_ms: 3.961051190951073\n",
            "  mean_raw_obs_processing_ms: 6.87827168104954\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 852.76\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3053679980710149\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [46, 375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000,\n",
            "      1000, 1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000,\n",
            "      711, 1000, 1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000,\n",
            "      962, 899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000,\n",
            "      1000, 1000, 1000, 530, 1000]\n",
            "    episode_reward: [-0.09119999408721924, -0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018,\n",
            "      0.0, -0.618399977684021, 0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036,\n",
            "      -1.5024000108242035, 0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012,\n",
            "      0.0, -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.420799970626831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0,\n",
            "      -0.7208000421524048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799,\n",
            "      0.0, -1.3191999793052673, 0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675,\n",
            "      0.0, 0.0, -1.9231999963521957, -1.7971999943256378, -0.09640002250671387, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.3087999820709229, -1.9472000002861023, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8955999612808228, -1.237999975681305,\n",
            "      0.0, 0.0, 0.0, -1.9635999985039234, 0.0, 0.0, 0.0, 0.0, -1.0587999820709229,\n",
            "      0.0]\n",
            "    policy_policy_0_reward: [0.9544000029563904, 0.9544000029563904, 0.625, 0.625,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [-1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482, 0.24879999458789825,\n",
            "      0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509,\n",
            "      0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.5685999989509583, 0.5685999989509583, 0.7770000100135803, 0.7770000100135803,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845, 0.2896000146865845,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9544000029563904\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.05458199987187982\n",
            "    policy_1: -0.09810199916362762\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15055429469556864\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306935556404629\n",
            "    mean_inference_ms: 3.961051190951073\n",
            "    mean_raw_obs_processing_ms: 6.87827168104954\n",
            "time_since_restore: 5652.580923318863\n",
            "time_this_iter_s: 5.962710857391357\n",
            "time_total_s: 5652.580923318863\n",
            "timers:\n",
            "  learn_throughput: 264.794\n",
            "  learn_time_ms: 966.789\n",
            "  synch_weights_time_ms: 5.897\n",
            "  training_iteration_time_ms: 5655.757\n",
            "timestamp: 1677558411\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 250624\n",
            "training_iteration: 979\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1003520\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1003520\n",
            "  num_agent_steps_trained: 1003520\n",
            "  num_env_steps_sampled: 250880\n",
            "  num_env_steps_trained: 250880\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-26-55\n",
            "done: false\n",
            "episode_len_mean: 852.76\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3053679980710149\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 307\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.98694948554039\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3338314674794673\n",
            "        kl: 0.019383365413492927\n",
            "        policy_loss: -0.11846383516676724\n",
            "        total_loss: -0.12735198839800432\n",
            "        vf_explained_var: 0.5350521549582481\n",
            "        vf_loss: 4.6506354829034534e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39180.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.61901678442955\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1149098858237267\n",
            "        kl: 0.015541171528620955\n",
            "        policy_loss: -0.11282968777231872\n",
            "        total_loss: -0.11908220178447664\n",
            "        vf_explained_var: 0.7057129859924316\n",
            "        vf_loss: 0.00011806064194388454\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39180.5\n",
            "  num_agent_steps_sampled: 1003520\n",
            "  num_agent_steps_trained: 1003520\n",
            "  num_env_steps_sampled: 250880\n",
            "  num_env_steps_trained: 250880\n",
            "iterations_since_restore: 980\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1003520\n",
            "num_agent_steps_trained: 1003520\n",
            "num_env_steps_sampled: 250880\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 250880\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 56.44285714285714\n",
            "  ram_util_percent: 37.800000000000004\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9544000029563904\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.05458199987187982\n",
            "  policy_1: -0.09810199916362762\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15055429469556864\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306935556404629\n",
            "  mean_inference_ms: 3.961051190951073\n",
            "  mean_raw_obs_processing_ms: 6.87827168104954\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 852.76\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3053679980710149\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [46, 375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000,\n",
            "      1000, 1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000,\n",
            "      711, 1000, 1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000,\n",
            "      962, 899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000,\n",
            "      1000, 1000, 1000, 530, 1000]\n",
            "    episode_reward: [-0.09119999408721924, -0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018,\n",
            "      0.0, -0.618399977684021, 0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036,\n",
            "      -1.5024000108242035, 0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012,\n",
            "      0.0, -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.420799970626831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0,\n",
            "      -0.7208000421524048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799,\n",
            "      0.0, -1.3191999793052673, 0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675,\n",
            "      0.0, 0.0, -1.9231999963521957, -1.7971999943256378, -0.09640002250671387, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.3087999820709229, -1.9472000002861023, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8955999612808228, -1.237999975681305,\n",
            "      0.0, 0.0, 0.0, -1.9635999985039234, 0.0, 0.0, 0.0, 0.0, -1.0587999820709229,\n",
            "      0.0]\n",
            "    policy_policy_0_reward: [0.9544000029563904, 0.9544000029563904, 0.625, 0.625,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [-1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482, 0.24879999458789825,\n",
            "      0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509,\n",
            "      0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.5685999989509583, 0.5685999989509583, 0.7770000100135803, 0.7770000100135803,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845, 0.2896000146865845,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9544000029563904\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.05458199987187982\n",
            "    policy_1: -0.09810199916362762\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15055429469556864\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306935556404629\n",
            "    mean_inference_ms: 3.961051190951073\n",
            "    mean_raw_obs_processing_ms: 6.87827168104954\n",
            "time_since_restore: 5657.41081571579\n",
            "time_this_iter_s: 4.82989239692688\n",
            "time_total_s: 5657.41081571579\n",
            "timers:\n",
            "  learn_throughput: 264.648\n",
            "  learn_time_ms: 967.322\n",
            "  synch_weights_time_ms: 5.897\n",
            "  training_iteration_time_ms: 5562.866\n",
            "timestamp: 1677558415\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 250880\n",
            "training_iteration: 980\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1004544\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1004544\n",
            "  num_agent_steps_trained: 1004544\n",
            "  num_env_steps_sampled: 251136\n",
            "  num_env_steps_trained: 251136\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-02\n",
            "done: false\n",
            "episode_len_mean: 852.76\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3053679980710149\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 307\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.9545498549938203\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3389324150979518\n",
            "        kl: 0.0187218284679322\n",
            "        policy_loss: -0.1175177067052573\n",
            "        total_loss: -0.12679794151335955\n",
            "        vf_explained_var: 0.7029646158218383\n",
            "        vf_loss: 2.7706493119694643e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39220.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.626547801494598\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.264209045469761\n",
            "        kl: 0.01538820033631263\n",
            "        policy_loss: -0.10909092547371983\n",
            "        total_loss: -0.11561499738600105\n",
            "        vf_explained_var: 0.7766979783773422\n",
            "        vf_loss: 9.000254331112956e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39220.5\n",
            "  num_agent_steps_sampled: 1004544\n",
            "  num_agent_steps_trained: 1004544\n",
            "  num_env_steps_sampled: 251136\n",
            "  num_env_steps_trained: 251136\n",
            "iterations_since_restore: 981\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1004544\n",
            "num_agent_steps_trained: 1004544\n",
            "num_env_steps_sampled: 251136\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 251136\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 85.06666666666666\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9544000029563904\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.05458199987187982\n",
            "  policy_1: -0.09810199916362762\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15055429469556864\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306935556404629\n",
            "  mean_inference_ms: 3.961051190951073\n",
            "  mean_raw_obs_processing_ms: 6.87827168104954\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 852.76\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3053679980710149\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [46, 375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000,\n",
            "      1000, 1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000,\n",
            "      711, 1000, 1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000,\n",
            "      962, 899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000,\n",
            "      1000, 1000, 1000, 530, 1000]\n",
            "    episode_reward: [-0.09119999408721924, -0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018,\n",
            "      0.0, -0.618399977684021, 0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036,\n",
            "      -1.5024000108242035, 0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012,\n",
            "      0.0, -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.420799970626831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0,\n",
            "      -0.7208000421524048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799,\n",
            "      0.0, -1.3191999793052673, 0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675,\n",
            "      0.0, 0.0, -1.9231999963521957, -1.7971999943256378, -0.09640002250671387, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.3087999820709229, -1.9472000002861023, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8955999612808228, -1.237999975681305,\n",
            "      0.0, 0.0, 0.0, -1.9635999985039234, 0.0, 0.0, 0.0, 0.0, -1.0587999820709229,\n",
            "      0.0]\n",
            "    policy_policy_0_reward: [0.9544000029563904, 0.9544000029563904, 0.625, 0.625,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [-1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482, 0.24879999458789825,\n",
            "      0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509,\n",
            "      0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.5685999989509583, 0.5685999989509583, 0.7770000100135803, 0.7770000100135803,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845, 0.2896000146865845,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9544000029563904\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.05458199987187982\n",
            "    policy_1: -0.09810199916362762\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15055429469556864\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306935556404629\n",
            "    mean_inference_ms: 3.961051190951073\n",
            "    mean_raw_obs_processing_ms: 6.87827168104954\n",
            "time_since_restore: 5663.468573093414\n",
            "time_this_iter_s: 6.057757377624512\n",
            "time_total_s: 5663.468573093414\n",
            "timers:\n",
            "  learn_throughput: 256.034\n",
            "  learn_time_ms: 999.865\n",
            "  synch_weights_time_ms: 6.297\n",
            "  training_iteration_time_ms: 5677.724\n",
            "timestamp: 1677558422\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 251136\n",
            "training_iteration: 981\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1005568\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1005568\n",
            "  num_agent_steps_trained: 1005568\n",
            "  num_env_steps_sampled: 251392\n",
            "  num_env_steps_trained: 251392\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-08\n",
            "done: false\n",
            "episode_len_mean: 862.3\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3044559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 1\n",
            "episodes_total: 308\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.9470427453517916\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 0.9646024070680141\n",
            "        kl: 0.01740125332537914\n",
            "        policy_loss: -0.11427908581681549\n",
            "        total_loss: -0.1249136911937967\n",
            "        vf_explained_var: 0.6475193202495575\n",
            "        vf_loss: 2.2523025040754874e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39260.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.697476637363434\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3944748416543007\n",
            "        kl: 0.016634405383287466\n",
            "        policy_loss: -0.11111963405273854\n",
            "        total_loss: -0.11675425227731466\n",
            "        vf_explained_var: 0.8021765619516372\n",
            "        vf_loss: 5.6628059701324675e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39260.5\n",
            "  num_agent_steps_sampled: 1005568\n",
            "  num_agent_steps_trained: 1005568\n",
            "  num_env_steps_sampled: 251392\n",
            "  num_env_steps_trained: 251392\n",
            "iterations_since_restore: 982\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1005568\n",
            "num_agent_steps_trained: 1005568\n",
            "num_env_steps_sampled: 251392\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 251392\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 71.61111111111111\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.06412599990144371\n",
            "  policy_1: -0.08810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15054796461311032\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306846406367014\n",
            "  mean_inference_ms: 3.960898838305174\n",
            "  mean_raw_obs_processing_ms: 6.878078034644995\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 862.3\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3044559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 1\n",
            "  hist_stats:\n",
            "    episode_lengths: [375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000,\n",
            "      1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962,\n",
            "      899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000,\n",
            "      1000, 1000, 530, 1000, 1000]\n",
            "    episode_reward: [-0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.625, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.4864000082015991, 0.4864000082015991, 0.0, 0.0, 0.6908000111579895, 0.6908000111579895,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38179999589920044, 0.38179999589920044,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0,\n",
            "      0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923,\n",
            "      0.0, 0.0, 0.8931999802589417, 0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19820000231266022, 0.19820000231266022, 0.0,\n",
            "      0.0, 0.6395999789237976, 0.6395999789237976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.10140000283718109, 0.10140000283718109, 0.9517999887466431, 0.9517999887466431,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386,\n",
            "      0.3456000089645386, 0.026399999856948853, 0.026399999856948853, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018200000748038292, 0.018200000748038292,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386,\n",
            "      0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [-1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.6412000060081482, 0.6412000060081482, 0.24879999458789825, 0.24879999458789825,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.06412599990144371\n",
            "    policy_1: -0.08810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15054796461311032\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306846406367014\n",
            "    mean_inference_ms: 3.960898838305174\n",
            "    mean_raw_obs_processing_ms: 6.878078034644995\n",
            "time_since_restore: 5669.636043787003\n",
            "time_this_iter_s: 6.167470693588257\n",
            "time_total_s: 5669.636043787003\n",
            "timers:\n",
            "  learn_throughput: 265.72\n",
            "  learn_time_ms: 963.419\n",
            "  synch_weights_time_ms: 6.039\n",
            "  training_iteration_time_ms: 5673.2\n",
            "timestamp: 1677558428\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 251392\n",
            "training_iteration: 982\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1006592\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1006592\n",
            "  num_agent_steps_trained: 1006592\n",
            "  num_env_steps_sampled: 251648\n",
            "  num_env_steps_trained: 251648\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-13\n",
            "done: false\n",
            "episode_len_mean: 862.3\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3044559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 308\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.9222870588302614\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.376073807477951\n",
            "        kl: 0.018301818389118974\n",
            "        policy_loss: -0.11136144488118589\n",
            "        total_loss: -0.12077368872705847\n",
            "        vf_explained_var: 0.5882259905338287\n",
            "        vf_loss: 1.932754662448133e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39300.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.7212459981441497\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1423830315470695\n",
            "        kl: 0.015680167323228633\n",
            "        policy_loss: -0.11386892986483872\n",
            "        total_loss: -0.120965426415205\n",
            "        vf_explained_var: 0.8004957184195518\n",
            "        vf_loss: 5.646115132549312e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39300.5\n",
            "  num_agent_steps_sampled: 1006592\n",
            "  num_agent_steps_trained: 1006592\n",
            "  num_env_steps_sampled: 251648\n",
            "  num_env_steps_trained: 251648\n",
            "iterations_since_restore: 983\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1006592\n",
            "num_agent_steps_trained: 1006592\n",
            "num_env_steps_sampled: 251648\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 251648\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 55.0\n",
            "  ram_util_percent: 37.800000000000004\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.06412599990144371\n",
            "  policy_1: -0.08810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15054796461311032\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306846406367014\n",
            "  mean_inference_ms: 3.960898838305174\n",
            "  mean_raw_obs_processing_ms: 6.878078034644995\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 862.3\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3044559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000,\n",
            "      1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962,\n",
            "      899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000,\n",
            "      1000, 1000, 530, 1000, 1000]\n",
            "    episode_reward: [-0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.625, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.4864000082015991, 0.4864000082015991, 0.0, 0.0, 0.6908000111579895, 0.6908000111579895,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38179999589920044, 0.38179999589920044,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0,\n",
            "      0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923,\n",
            "      0.0, 0.0, 0.8931999802589417, 0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19820000231266022, 0.19820000231266022, 0.0,\n",
            "      0.0, 0.6395999789237976, 0.6395999789237976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.10140000283718109, 0.10140000283718109, 0.9517999887466431, 0.9517999887466431,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386,\n",
            "      0.3456000089645386, 0.026399999856948853, 0.026399999856948853, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018200000748038292, 0.018200000748038292,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386,\n",
            "      0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [-1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.6412000060081482, 0.6412000060081482, 0.24879999458789825, 0.24879999458789825,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.06412599990144371\n",
            "    policy_1: -0.08810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15054796461311032\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306846406367014\n",
            "    mean_inference_ms: 3.960898838305174\n",
            "    mean_raw_obs_processing_ms: 6.878078034644995\n",
            "time_since_restore: 5674.410933494568\n",
            "time_this_iter_s: 4.774889707565308\n",
            "time_total_s: 5674.410933494568\n",
            "timers:\n",
            "  learn_throughput: 266.625\n",
            "  learn_time_ms: 960.151\n",
            "  synch_weights_time_ms: 6.045\n",
            "  training_iteration_time_ms: 5565.936\n",
            "timestamp: 1677558433\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 251648\n",
            "training_iteration: 983\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1007616\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1007616\n",
            "  num_agent_steps_trained: 1007616\n",
            "  num_env_steps_sampled: 251904\n",
            "  num_env_steps_trained: 251904\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-19\n",
            "done: false\n",
            "episode_len_mean: 862.3\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.3044559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 308\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.925187611579895\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3044772878289224\n",
            "        kl: 0.018941591603098627\n",
            "        policy_loss: -0.10937803434208035\n",
            "        total_loss: -0.11812676591798663\n",
            "        vf_explained_var: 0.7101492047309875\n",
            "        vf_loss: 9.371830037707695e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39340.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.699617612361908\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.2171805381774903\n",
            "        kl: 0.017344703033220065\n",
            "        policy_loss: -0.1158742436207831\n",
            "        total_loss: -0.12061903486028314\n",
            "        vf_explained_var: 0.7595965713262558\n",
            "        vf_loss: 3.779537501031882e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39340.5\n",
            "  num_agent_steps_sampled: 1007616\n",
            "  num_agent_steps_trained: 1007616\n",
            "  num_env_steps_sampled: 251904\n",
            "  num_env_steps_trained: 251904\n",
            "iterations_since_restore: 984\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1007616\n",
            "num_agent_steps_trained: 1007616\n",
            "num_env_steps_sampled: 251904\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 251904\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 86.54444444444444\n",
            "  ram_util_percent: 37.82222222222222\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.06412599990144371\n",
            "  policy_1: -0.08810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15054796461311032\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306846406367014\n",
            "  mean_inference_ms: 3.960898838305174\n",
            "  mean_raw_obs_processing_ms: 6.878078034644995\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 862.3\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.3044559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [375, 1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000,\n",
            "      1000, 618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962,\n",
            "      899, 48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000,\n",
            "      1000, 1000, 530, 1000, 1000]\n",
            "    episode_reward: [-0.75, 0.0, 0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.625, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.4864000082015991, 0.4864000082015991, 0.0, 0.0, 0.6908000111579895, 0.6908000111579895,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38179999589920044, 0.38179999589920044,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0,\n",
            "      0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923,\n",
            "      0.0, 0.0, 0.8931999802589417, 0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19820000231266022, 0.19820000231266022, 0.0,\n",
            "      0.0, 0.6395999789237976, 0.6395999789237976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.10140000283718109, 0.10140000283718109, 0.9517999887466431, 0.9517999887466431,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386,\n",
            "      0.3456000089645386, 0.026399999856948853, 0.026399999856948853, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018200000748038292, 0.018200000748038292,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386,\n",
            "      0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [-1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.6412000060081482, 0.6412000060081482, 0.24879999458789825, 0.24879999458789825,\n",
            "      0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.06412599990144371\n",
            "    policy_1: -0.08810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15054796461311032\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306846406367014\n",
            "    mean_inference_ms: 3.960898838305174\n",
            "    mean_raw_obs_processing_ms: 6.878078034644995\n",
            "time_since_restore: 5680.4438643455505\n",
            "time_this_iter_s: 6.032930850982666\n",
            "time_total_s: 5680.4438643455505\n",
            "timers:\n",
            "  learn_throughput: 257.216\n",
            "  learn_time_ms: 995.274\n",
            "  synch_weights_time_ms: 6.287\n",
            "  training_iteration_time_ms: 5681.249\n",
            "timestamp: 1677558439\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 251904\n",
            "training_iteration: 984\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1008640\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1008640\n",
            "  num_agent_steps_trained: 1008640\n",
            "  num_env_steps_sampled: 252160\n",
            "  num_env_steps_trained: 252160\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-25\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 1\n",
            "episodes_total: 309\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.9649025738239287\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.0518525563180448\n",
            "        kl: 0.01705080191549958\n",
            "        policy_loss: -0.11644015302881598\n",
            "        total_loss: -0.12763270582072436\n",
            "        vf_explained_var: 0.6581309139728546\n",
            "        vf_loss: 8.952470159329095e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39380.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.6356051683425905\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.442817885428667\n",
            "        kl: 0.017805541610360187\n",
            "        policy_loss: -0.10933145368471742\n",
            "        total_loss: -0.11284491268452257\n",
            "        vf_explained_var: 0.7090349957346916\n",
            "        vf_loss: 3.253445820519119e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39380.5\n",
            "  num_agent_steps_sampled: 1008640\n",
            "  num_agent_steps_trained: 1008640\n",
            "  num_env_steps_sampled: 252160\n",
            "  num_env_steps_trained: 252160\n",
            "iterations_since_restore: 985\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1008640\n",
            "num_agent_steps_trained: 1008640\n",
            "num_env_steps_sampled: 252160\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 252160\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 79.83333333333333\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15054153339702073\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306753724211247\n",
            "  mean_inference_ms: 3.960741746745781\n",
            "  mean_raw_obs_processing_ms: 6.877878491571556\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 1\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000,\n",
            "      618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899,\n",
            "      48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991,\n",
            "      0.4864000082015991, 0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0,\n",
            "      0.0, 0.8931999802589417, 0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.19820000231266022, 0.19820000231266022, 0.0, 0.0,\n",
            "      0.6395999789237976, 0.6395999789237976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.10140000283718109, 0.10140000283718109, 0.9517999887466431, 0.9517999887466431,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386,\n",
            "      0.3456000089645386, 0.026399999856948853, 0.026399999856948853, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018200000748038292, 0.018200000748038292,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482,\n",
            "      0.6412000060081482, 0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15054153339702073\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306753724211247\n",
            "    mean_inference_ms: 3.960741746745781\n",
            "    mean_raw_obs_processing_ms: 6.877878491571556\n",
            "time_since_restore: 5686.642730712891\n",
            "time_this_iter_s: 6.198866367340088\n",
            "time_total_s: 5686.642730712891\n",
            "timers:\n",
            "  learn_throughput: 267.139\n",
            "  learn_time_ms: 958.304\n",
            "  synch_weights_time_ms: 6.238\n",
            "  training_iteration_time_ms: 5674.228\n",
            "timestamp: 1677558445\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 252160\n",
            "training_iteration: 985\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1009664\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1009664\n",
            "  num_agent_steps_trained: 1009664\n",
            "  num_env_steps_sampled: 252416\n",
            "  num_env_steps_trained: 252416\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-30\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 309\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 3.0068558037281035\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.5299204722046853\n",
            "        kl: 0.018483774903247242\n",
            "        policy_loss: -0.11309808576479555\n",
            "        total_loss: -0.12315905541181564\n",
            "        vf_explained_var: 0.6821409493684769\n",
            "        vf_loss: 6.7923927360880045e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39420.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.685169440507889\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.0962441965937615\n",
            "        kl: 0.014404976626847033\n",
            "        policy_loss: -0.11412609019316733\n",
            "        total_loss: -0.12249774865340442\n",
            "        vf_explained_var: 0.743033304810524\n",
            "        vf_loss: 2.695811372177559e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39420.5\n",
            "  num_agent_steps_sampled: 1009664\n",
            "  num_agent_steps_trained: 1009664\n",
            "  num_env_steps_sampled: 252416\n",
            "  num_env_steps_trained: 252416\n",
            "iterations_since_restore: 986\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1009664\n",
            "num_agent_steps_trained: 1009664\n",
            "num_env_steps_sampled: 252416\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 252416\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 58.300000000000004\n",
            "  ram_util_percent: 37.800000000000004\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15054153339702073\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306753724211247\n",
            "  mean_inference_ms: 3.960741746745781\n",
            "  mean_raw_obs_processing_ms: 6.877878491571556\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000,\n",
            "      618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899,\n",
            "      48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991,\n",
            "      0.4864000082015991, 0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0,\n",
            "      0.0, 0.8931999802589417, 0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.19820000231266022, 0.19820000231266022, 0.0, 0.0,\n",
            "      0.6395999789237976, 0.6395999789237976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.10140000283718109, 0.10140000283718109, 0.9517999887466431, 0.9517999887466431,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386,\n",
            "      0.3456000089645386, 0.026399999856948853, 0.026399999856948853, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018200000748038292, 0.018200000748038292,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482,\n",
            "      0.6412000060081482, 0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15054153339702073\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306753724211247\n",
            "    mean_inference_ms: 3.960741746745781\n",
            "    mean_raw_obs_processing_ms: 6.877878491571556\n",
            "time_since_restore: 5691.517208814621\n",
            "time_this_iter_s: 4.874478101730347\n",
            "time_total_s: 5691.517208814621\n",
            "timers:\n",
            "  learn_throughput: 266.315\n",
            "  learn_time_ms: 961.269\n",
            "  synch_weights_time_ms: 6.332\n",
            "  training_iteration_time_ms: 5579.687\n",
            "timestamp: 1677558450\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 252416\n",
            "training_iteration: 986\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1010688\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1010688\n",
            "  num_agent_steps_trained: 1010688\n",
            "  num_env_steps_sampled: 252672\n",
            "  num_env_steps_trained: 252672\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-36\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 309\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.9373269379138947\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.0518090330064296\n",
            "        kl: 0.017418891313875796\n",
            "        policy_loss: -0.1144522501155734\n",
            "        total_loss: -0.12497060627210885\n",
            "        vf_explained_var: 0.5909168854355812\n",
            "        vf_loss: 3.918146586556759e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39460.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.659948492050171\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1990216016769408\n",
            "        kl: 0.01488483170329009\n",
            "        policy_loss: -0.11024457085877656\n",
            "        total_loss: -0.11774841797305272\n",
            "        vf_explained_var: 0.8712904766201973\n",
            "        vf_loss: 1.2686646107340493e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39460.5\n",
            "  num_agent_steps_sampled: 1010688\n",
            "  num_agent_steps_trained: 1010688\n",
            "  num_env_steps_sampled: 252672\n",
            "  num_env_steps_trained: 252672\n",
            "iterations_since_restore: 987\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1010688\n",
            "num_agent_steps_trained: 1010688\n",
            "num_env_steps_sampled: 252672\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 252672\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 84.22222222222223\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15054153339702073\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306753724211247\n",
            "  mean_inference_ms: 3.960741746745781\n",
            "  mean_raw_obs_processing_ms: 6.877878491571556\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000,\n",
            "      618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899,\n",
            "      48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991,\n",
            "      0.4864000082015991, 0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0,\n",
            "      0.0, 0.8931999802589417, 0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.19820000231266022, 0.19820000231266022, 0.0, 0.0,\n",
            "      0.6395999789237976, 0.6395999789237976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.10140000283718109, 0.10140000283718109, 0.9517999887466431, 0.9517999887466431,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386,\n",
            "      0.3456000089645386, 0.026399999856948853, 0.026399999856948853, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018200000748038292, 0.018200000748038292,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482,\n",
            "      0.6412000060081482, 0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15054153339702073\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306753724211247\n",
            "    mean_inference_ms: 3.960741746745781\n",
            "    mean_raw_obs_processing_ms: 6.877878491571556\n",
            "time_since_restore: 5697.559438943863\n",
            "time_this_iter_s: 6.042230129241943\n",
            "time_total_s: 5697.559438943863\n",
            "timers:\n",
            "  learn_throughput: 257.992\n",
            "  learn_time_ms: 992.28\n",
            "  synch_weights_time_ms: 6.808\n",
            "  training_iteration_time_ms: 5700.684\n",
            "timestamp: 1677558456\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 252672\n",
            "training_iteration: 987\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1011712\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1011712\n",
            "  num_agent_steps_trained: 1011712\n",
            "  num_env_steps_sampled: 252928\n",
            "  num_env_steps_trained: 252928\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-42\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 309\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.996378427743912\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.4369772896170616\n",
            "        kl: 0.01943080383371463\n",
            "        policy_loss: -0.10904815047979355\n",
            "        total_loss: -0.11797925065038725\n",
            "        vf_explained_var: 0.6385635539889336\n",
            "        vf_loss: 5.398860497507485e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39500.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.7416407763957977\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3068112157285214\n",
            "        kl: 0.016407859808971324\n",
            "        policy_loss: -0.10917871370911598\n",
            "        total_loss: -0.11554560316726566\n",
            "        vf_explained_var: 0.7840053349733352\n",
            "        vf_loss: 1.4965204741201887e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39500.5\n",
            "  num_agent_steps_sampled: 1011712\n",
            "  num_agent_steps_trained: 1011712\n",
            "  num_env_steps_sampled: 252928\n",
            "  num_env_steps_trained: 252928\n",
            "iterations_since_restore: 988\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1011712\n",
            "num_agent_steps_trained: 1011712\n",
            "num_env_steps_sampled: 252928\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 252928\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 79.3875\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15054153339702073\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306753724211247\n",
            "  mean_inference_ms: 3.960741746745781\n",
            "  mean_raw_obs_processing_ms: 6.877878491571556\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000,\n",
            "      618, 359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899,\n",
            "      48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991,\n",
            "      0.4864000082015991, 0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0,\n",
            "      0.0, 0.8931999802589417, 0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.19820000231266022, 0.19820000231266022, 0.0, 0.0,\n",
            "      0.6395999789237976, 0.6395999789237976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.10140000283718109, 0.10140000283718109, 0.9517999887466431, 0.9517999887466431,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386,\n",
            "      0.3456000089645386, 0.026399999856948853, 0.026399999856948853, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018200000748038292, 0.018200000748038292,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482,\n",
            "      0.6412000060081482, 0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15054153339702073\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306753724211247\n",
            "    mean_inference_ms: 3.960741746745781\n",
            "    mean_raw_obs_processing_ms: 6.877878491571556\n",
            "time_since_restore: 5703.464014291763\n",
            "time_this_iter_s: 5.904575347900391\n",
            "time_total_s: 5703.464014291763\n",
            "timers:\n",
            "  learn_throughput: 266.76\n",
            "  learn_time_ms: 959.665\n",
            "  synch_weights_time_ms: 6.701\n",
            "  training_iteration_time_ms: 5680.284\n",
            "timestamp: 1677558462\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 252928\n",
            "training_iteration: 988\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1012736\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1012736\n",
            "  num_agent_steps_trained: 1012736\n",
            "  num_env_steps_sampled: 253184\n",
            "  num_env_steps_trained: 253184\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-47\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 1\n",
            "episodes_total: 310\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.9512578189373015\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.0700874626636505\n",
            "        kl: 0.017177064433008215\n",
            "        policy_loss: -0.10424207821488381\n",
            "        total_loss: -0.11516150622628629\n",
            "        vf_explained_var: 0.4827766939997673\n",
            "        vf_loss: 4.720488101384035e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39540.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.6910758912563324\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.2239584922790527\n",
            "        kl: 0.015068151134118035\n",
            "        policy_loss: -0.10232901899144053\n",
            "        total_loss: -0.1099089621566236\n",
            "        vf_explained_var: 0.9035445705056191\n",
            "        vf_loss: 1.3026568763052637e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39540.5\n",
            "  num_agent_steps_sampled: 1012736\n",
            "  num_agent_steps_trained: 1012736\n",
            "  num_env_steps_sampled: 253184\n",
            "  num_env_steps_trained: 253184\n",
            "iterations_since_restore: 989\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1012736\n",
            "num_agent_steps_trained: 1012736\n",
            "num_env_steps_sampled: 253184\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 253184\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 48.92857142857143\n",
            "  ram_util_percent: 37.800000000000004\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15053541067880538\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306667325187811\n",
            "  mean_inference_ms: 3.9605909319990134\n",
            "  mean_raw_obs_processing_ms: 6.877691325055383\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 1\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618,\n",
            "      359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899,\n",
            "      48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482,\n",
            "      0.6412000060081482, 0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15053541067880538\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306667325187811\n",
            "    mean_inference_ms: 3.9605909319990134\n",
            "    mean_raw_obs_processing_ms: 6.877691325055383\n",
            "time_since_restore: 5708.56299328804\n",
            "time_this_iter_s: 5.0989789962768555\n",
            "time_total_s: 5708.56299328804\n",
            "timers:\n",
            "  learn_throughput: 265.876\n",
            "  learn_time_ms: 962.856\n",
            "  synch_weights_time_ms: 6.666\n",
            "  training_iteration_time_ms: 5593.943\n",
            "timestamp: 1677558467\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 253184\n",
            "training_iteration: 989\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1013760\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1013760\n",
            "  num_agent_steps_trained: 1013760\n",
            "  num_env_steps_sampled: 253440\n",
            "  num_env_steps_trained: 253440\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-53\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 310\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.949302750825882\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1880687952041626\n",
            "        kl: 0.018859929252239914\n",
            "        policy_loss: -0.10915430136956275\n",
            "        total_loss: -0.11823258032090962\n",
            "        vf_explained_var: 0.5608320638537407\n",
            "        vf_loss: 2.173619259338011e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39580.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.6672248423099516\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.5348868176341057\n",
            "        kl: 0.018754459250165344\n",
            "        policy_loss: -0.1133100860286504\n",
            "        total_loss: -0.11592238140292466\n",
            "        vf_explained_var: 0.8025612741708755\n",
            "        vf_loss: 1.117425194934185e-05\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39580.5\n",
            "  num_agent_steps_sampled: 1013760\n",
            "  num_agent_steps_trained: 1013760\n",
            "  num_env_steps_sampled: 253440\n",
            "  num_env_steps_trained: 253440\n",
            "iterations_since_restore: 990\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1013760\n",
            "num_agent_steps_trained: 1013760\n",
            "num_env_steps_sampled: 253440\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 253440\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 62.599999999999994\n",
            "  ram_util_percent: 37.81111111111111\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15053541067880538\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306667325187811\n",
            "  mean_inference_ms: 3.9605909319990134\n",
            "  mean_raw_obs_processing_ms: 6.877691325055383\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618,\n",
            "      359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899,\n",
            "      48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482,\n",
            "      0.6412000060081482, 0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15053541067880538\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306667325187811\n",
            "    mean_inference_ms: 3.9605909319990134\n",
            "    mean_raw_obs_processing_ms: 6.877691325055383\n",
            "time_since_restore: 5714.476859807968\n",
            "time_this_iter_s: 5.9138665199279785\n",
            "time_total_s: 5714.476859807968\n",
            "timers:\n",
            "  learn_throughput: 257.533\n",
            "  learn_time_ms: 994.048\n",
            "  synch_weights_time_ms: 6.874\n",
            "  training_iteration_time_ms: 5701.875\n",
            "timestamp: 1677558473\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 253440\n",
            "training_iteration: 990\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1014784\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1014784\n",
            "  num_agent_steps_trained: 1014784\n",
            "  num_env_steps_sampled: 253696\n",
            "  num_env_steps_trained: 253696\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-27-59\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 310\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.8769539058208466\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1747051052749158\n",
            "        kl: 0.01812005840719406\n",
            "        policy_loss: -0.11615572248119861\n",
            "        total_loss: -0.12531137764453887\n",
            "        vf_explained_var: 0.6947282448410987\n",
            "        vf_loss: 1.965408571891203e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39620.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.743510925769806\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.492669652402401\n",
            "        kl: 0.016239846203524123\n",
            "        policy_loss: -0.10667787091806531\n",
            "        total_loss: -0.11327900970354676\n",
            "        vf_explained_var: 0.7938445881009102\n",
            "        vf_loss: 8.683518080943031e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39620.5\n",
            "  num_agent_steps_sampled: 1014784\n",
            "  num_agent_steps_trained: 1014784\n",
            "  num_env_steps_sampled: 253696\n",
            "  num_env_steps_trained: 253696\n",
            "iterations_since_restore: 991\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1014784\n",
            "num_agent_steps_trained: 1014784\n",
            "num_env_steps_sampled: 253696\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 253696\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 13.7625\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15053541067880538\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306667325187811\n",
            "  mean_inference_ms: 3.9605909319990134\n",
            "  mean_raw_obs_processing_ms: 6.877691325055383\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618,\n",
            "      359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899,\n",
            "      48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482,\n",
            "      0.6412000060081482, 0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15053541067880538\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306667325187811\n",
            "    mean_inference_ms: 3.9605909319990134\n",
            "    mean_raw_obs_processing_ms: 6.877691325055383\n",
            "time_since_restore: 5720.332646369934\n",
            "time_this_iter_s: 5.855786561965942\n",
            "time_total_s: 5720.332646369934\n",
            "timers:\n",
            "  learn_throughput: 266.499\n",
            "  learn_time_ms: 960.603\n",
            "  synch_weights_time_ms: 6.525\n",
            "  training_iteration_time_ms: 5682.137\n",
            "timestamp: 1677558479\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 253696\n",
            "training_iteration: 991\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1015808\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1015808\n",
            "  num_agent_steps_trained: 1015808\n",
            "  num_env_steps_sampled: 253952\n",
            "  num_env_steps_trained: 253952\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-04\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 310\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.8838472068309784\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3064543068408967\n",
            "        kl: 0.018947802178571303\n",
            "        policy_loss: -0.11406834982335567\n",
            "        total_loss: -0.12239695340394974\n",
            "        vf_explained_var: 0.8365994915366173\n",
            "        vf_loss: 1.3548586821343633e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39660.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.5438582241535186\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3442845910787582\n",
            "        kl: 0.016253798516914598\n",
            "        policy_loss: -0.10596708243247122\n",
            "        total_loss: -0.11055379046592861\n",
            "        vf_explained_var: 0.7752783030271531\n",
            "        vf_loss: 9.942258350292832e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39660.5\n",
            "  num_agent_steps_sampled: 1015808\n",
            "  num_agent_steps_trained: 1015808\n",
            "  num_env_steps_sampled: 253952\n",
            "  num_env_steps_trained: 253952\n",
            "iterations_since_restore: 992\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1015808\n",
            "num_agent_steps_trained: 1015808\n",
            "num_env_steps_sampled: 253952\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 253952\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 34.35\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15053541067880538\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306667325187811\n",
            "  mean_inference_ms: 3.9605909319990134\n",
            "  mean_raw_obs_processing_ms: 6.877691325055383\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618,\n",
            "      359, 751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899,\n",
            "      48, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482,\n",
            "      0.6412000060081482, 0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.7202000021934509, 0.7202000021934509, 0.2223999947309494,\n",
            "      0.2223999947309494, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583,\n",
            "      0.7770000100135803, 0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.2896000146865845, 0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101,\n",
            "      0.0, 0.0, 0.34040001034736633, 0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368,\n",
            "      0.6934000253677368, 0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15053541067880538\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306667325187811\n",
            "    mean_inference_ms: 3.9605909319990134\n",
            "    mean_raw_obs_processing_ms: 6.877691325055383\n",
            "time_since_restore: 5725.426852226257\n",
            "time_this_iter_s: 5.094205856323242\n",
            "time_total_s: 5725.426852226257\n",
            "timers:\n",
            "  learn_throughput: 266.669\n",
            "  learn_time_ms: 959.991\n",
            "  synch_weights_time_ms: 6.481\n",
            "  training_iteration_time_ms: 5574.842\n",
            "timestamp: 1677558484\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 253952\n",
            "training_iteration: 992\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1016832\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1016832\n",
            "  num_agent_steps_trained: 1016832\n",
            "  num_env_steps_sampled: 254208\n",
            "  num_env_steps_trained: 254208\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-10\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 1\n",
            "episodes_total: 311\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.943938505649567\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1260721564292908\n",
            "        kl: 0.01764164148042262\n",
            "        policy_loss: -0.10672561950050294\n",
            "        total_loss: -0.1170689805643633\n",
            "        vf_explained_var: 0.7464973345398903\n",
            "        vf_loss: 1.9883751491533984e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39700.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.5840624809265136\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.3528197951614858\n",
            "        kl: 0.016074391729787862\n",
            "        policy_loss: -0.10550733932759612\n",
            "        total_loss: -0.11072625312954187\n",
            "        vf_explained_var: 0.7527479559183121\n",
            "        vf_loss: 8.441908232725836e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39700.5\n",
            "  num_agent_steps_sampled: 1016832\n",
            "  num_agent_steps_trained: 1016832\n",
            "  num_env_steps_sampled: 254208\n",
            "  num_env_steps_trained: 254208\n",
            "iterations_since_restore: 993\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1016832\n",
            "num_agent_steps_trained: 1016832\n",
            "num_env_steps_sampled: 254208\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 254208\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 26.5375\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15052661967027936\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306536345051132\n",
            "  mean_inference_ms: 3.9604069795218813\n",
            "  mean_raw_obs_processing_ms: 6.8774326070987835\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 1\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618, 359,\n",
            "      751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899, 48,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021, 0.0,\n",
            "      0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482,\n",
            "      0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509,\n",
            "      0.7202000021934509, 0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583, 0.7770000100135803,\n",
            "      0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845,\n",
            "      0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15052661967027936\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306536345051132\n",
            "    mean_inference_ms: 3.9604069795218813\n",
            "    mean_raw_obs_processing_ms: 6.8774326070987835\n",
            "time_since_restore: 5731.3383774757385\n",
            "time_this_iter_s: 5.911525249481201\n",
            "time_total_s: 5731.3383774757385\n",
            "timers:\n",
            "  learn_throughput: 257.777\n",
            "  learn_time_ms: 993.105\n",
            "  synch_weights_time_ms: 6.688\n",
            "  training_iteration_time_ms: 5688.137\n",
            "timestamp: 1677558490\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 254208\n",
            "training_iteration: 993\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1017856\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1017856\n",
            "  num_agent_steps_trained: 1017856\n",
            "  num_env_steps_sampled: 254464\n",
            "  num_env_steps_trained: 254464\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-16\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 311\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.929992067813873\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1730203300714492\n",
            "        kl: 0.018039187941107503\n",
            "        policy_loss: -0.11073811969254166\n",
            "        total_loss: -0.12051169702317566\n",
            "        vf_explained_var: 0.7726724505424499\n",
            "        vf_loss: 1.0301893283326535e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39740.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.649026966094971\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.420486867427826\n",
            "        kl: 0.01528261640104791\n",
            "        policy_loss: -0.10684275506064296\n",
            "        total_loss: -0.11372707644477487\n",
            "        vf_explained_var: 0.8080890581011773\n",
            "        vf_loss: 4.965441678450588e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39740.5\n",
            "  num_agent_steps_sampled: 1017856\n",
            "  num_agent_steps_trained: 1017856\n",
            "  num_env_steps_sampled: 254464\n",
            "  num_env_steps_trained: 254464\n",
            "iterations_since_restore: 994\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1017856\n",
            "num_agent_steps_trained: 1017856\n",
            "num_env_steps_sampled: 254464\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 254464\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 11.11111111111111\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15052661967027936\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306536345051132\n",
            "  mean_inference_ms: 3.9604069795218813\n",
            "  mean_raw_obs_processing_ms: 6.8774326070987835\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618, 359,\n",
            "      751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899, 48,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021, 0.0,\n",
            "      0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482,\n",
            "      0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509,\n",
            "      0.7202000021934509, 0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583, 0.7770000100135803,\n",
            "      0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845,\n",
            "      0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15052661967027936\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306536345051132\n",
            "    mean_inference_ms: 3.9604069795218813\n",
            "    mean_raw_obs_processing_ms: 6.8774326070987835\n",
            "time_since_restore: 5737.307838201523\n",
            "time_this_iter_s: 5.969460725784302\n",
            "time_total_s: 5737.307838201523\n",
            "timers:\n",
            "  learn_throughput: 267.421\n",
            "  learn_time_ms: 957.292\n",
            "  synch_weights_time_ms: 6.427\n",
            "  training_iteration_time_ms: 5682.211\n",
            "timestamp: 1677558496\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 254464\n",
            "training_iteration: 994\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1018880\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1018880\n",
            "  num_agent_steps_trained: 1018880\n",
            "  num_env_steps_sampled: 254720\n",
            "  num_env_steps_trained: 254720\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-21\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 311\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.885630559921265\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.210911040008068\n",
            "        kl: 0.017510509644683703\n",
            "        policy_loss: -0.11016437090002\n",
            "        total_loss: -0.1200665945187211\n",
            "        vf_explained_var: 0.730161790549755\n",
            "        vf_loss: 1.1084755399792812e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39780.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.670223259925842\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1467106610536575\n",
            "        kl: 0.016450319278424618\n",
            "        policy_loss: -0.11327697280794383\n",
            "        total_loss: -0.11887521757744253\n",
            "        vf_explained_var: 0.8070688501000405\n",
            "        vf_loss: 4.962846969647217e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39780.5\n",
            "  num_agent_steps_sampled: 1018880\n",
            "  num_agent_steps_trained: 1018880\n",
            "  num_env_steps_sampled: 254720\n",
            "  num_env_steps_trained: 254720\n",
            "iterations_since_restore: 995\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1018880\n",
            "num_agent_steps_trained: 1018880\n",
            "num_env_steps_sampled: 254720\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 254720\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 14.285714285714286\n",
            "  ram_util_percent: 37.800000000000004\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15052661967027936\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306536345051132\n",
            "  mean_inference_ms: 3.9604069795218813\n",
            "  mean_raw_obs_processing_ms: 6.8774326070987835\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618, 359,\n",
            "      751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899, 48,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021, 0.0,\n",
            "      0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482,\n",
            "      0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509,\n",
            "      0.7202000021934509, 0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583, 0.7770000100135803,\n",
            "      0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845,\n",
            "      0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15052661967027936\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306536345051132\n",
            "    mean_inference_ms: 3.9604069795218813\n",
            "    mean_raw_obs_processing_ms: 6.8774326070987835\n",
            "time_since_restore: 5742.441574811935\n",
            "time_this_iter_s: 5.133736610412598\n",
            "time_total_s: 5742.441574811935\n",
            "timers:\n",
            "  learn_throughput: 267.146\n",
            "  learn_time_ms: 958.278\n",
            "  synch_weights_time_ms: 6.407\n",
            "  training_iteration_time_ms: 5575.69\n",
            "timestamp: 1677558501\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 254720\n",
            "training_iteration: 995\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1019904\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1019904\n",
            "  num_agent_steps_trained: 1019904\n",
            "  num_env_steps_sampled: 254976\n",
            "  num_env_steps_trained: 254976\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-27\n",
            "done: false\n",
            "episode_len_mean: 868.55\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.2969559981301427\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 311\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.8941161572933196\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 0.9799314647912979\n",
            "        kl: 0.019680002788208518\n",
            "        policy_loss: -0.12208679844625295\n",
            "        total_loss: -0.1297255298588425\n",
            "        vf_explained_var: 0.7326732143759728\n",
            "        vf_loss: 1.8496443601634382e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39820.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.707401692867279\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.2919451370835304\n",
            "        kl: 0.015901141974829613\n",
            "        policy_loss: -0.11214380483143031\n",
            "        total_loss: -0.11881837123073638\n",
            "        vf_explained_var: 0.7959305480122566\n",
            "        vf_loss: 4.691878547191663e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39820.5\n",
            "  num_agent_steps_sampled: 1019904\n",
            "  num_agent_steps_trained: 1019904\n",
            "  num_env_steps_sampled: 254976\n",
            "  num_env_steps_trained: 254976\n",
            "iterations_since_restore: 996\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1019904\n",
            "num_agent_steps_trained: 1019904\n",
            "num_env_steps_sampled: 254976\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 254976\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 72.05555555555556\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.07037599990144372\n",
            "  policy_1: -0.07810199916362763\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15052661967027936\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306536345051132\n",
            "  mean_inference_ms: 3.9604069795218813\n",
            "  mean_raw_obs_processing_ms: 6.8774326070987835\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.2969559981301427\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618, 359,\n",
            "      751, 1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      323, 1000, 660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899, 48,\n",
            "      1000, 1000, 1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000,\n",
            "      1000, 530, 1000, 1000, 1000, 1000, 1000]\n",
            "    episode_reward: [0.0, 0.0, -1.0271999835968018, 0.0, -0.618399977684021, 0.0,\n",
            "      0.0, 0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035,\n",
            "      0.0, -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0,\n",
            "      -0.13399994373321533, 0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -0.8628000020980835, -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673,\n",
            "      0.0, 0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.0, 0.0, 0.4864000082015991, 0.4864000082015991,\n",
            "      0.0, 0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482,\n",
            "      0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509,\n",
            "      0.7202000021934509, 0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583, 0.7770000100135803,\n",
            "      0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845,\n",
            "      0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.07037599990144372\n",
            "    policy_1: -0.07810199916362763\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15052661967027936\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306536345051132\n",
            "    mean_inference_ms: 3.9604069795218813\n",
            "    mean_raw_obs_processing_ms: 6.8774326070987835\n",
            "time_since_restore: 5748.268484354019\n",
            "time_this_iter_s: 5.82690954208374\n",
            "time_total_s: 5748.268484354019\n",
            "timers:\n",
            "  learn_throughput: 258.824\n",
            "  learn_time_ms: 989.089\n",
            "  synch_weights_time_ms: 6.365\n",
            "  training_iteration_time_ms: 5670.551\n",
            "timestamp: 1677558507\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 254976\n",
            "training_iteration: 996\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1020928\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1020928\n",
            "  num_agent_steps_trained: 1020928\n",
            "  num_env_steps_sampled: 255232\n",
            "  num_env_steps_trained: 255232\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-34\n",
            "done: false\n",
            "episode_len_mean: 868.25\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.31635599814355375\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 1\n",
            "episodes_total: 312\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.938361817598343\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 0.6126056309789419\n",
            "        kl: 0.009789277563685217\n",
            "        policy_loss: -0.013203533180058003\n",
            "        total_loss: -0.03199054291471839\n",
            "        vf_explained_var: -0.3106191024184227\n",
            "        vf_loss: 0.002968030955318568\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39860.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.6513298571109774\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.02204003110528\n",
            "        kl: 0.01457658506004873\n",
            "        policy_loss: -0.08704890690278262\n",
            "        total_loss: -0.09486201698891819\n",
            "        vf_explained_var: 0.6061046734452248\n",
            "        vf_loss: 8.914424944350685e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39860.5\n",
            "  num_agent_steps_sampled: 1020928\n",
            "  num_agent_steps_trained: 1020928\n",
            "  num_env_steps_sampled: 255232\n",
            "  num_env_steps_trained: 255232\n",
            "iterations_since_restore: 997\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1020928\n",
            "num_agent_steps_trained: 1020928\n",
            "num_env_steps_sampled: 255232\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 255232\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 99.58\n",
            "  ram_util_percent: 37.83\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.08037599990144371\n",
            "  policy_1: -0.07780199917033315\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15051849552772423\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306416200745006\n",
            "  mean_inference_ms: 3.960227731741272\n",
            "  mean_raw_obs_processing_ms: 6.8771886765413806\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.31635599814355375\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 1\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618, 359, 751,\n",
            "      1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 323, 1000,\n",
            "      660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899, 48, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000, 1000, 530,\n",
            "      1000, 1000, 1000, 1000, 1000, 970]\n",
            "    episode_reward: [0.0, -1.0271999835968018, 0.0, -0.618399977684021, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035, 0.0,\n",
            "      -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0, -0.13399994373321533,\n",
            "      0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8628000020980835,\n",
            "      -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673, 0.0,\n",
            "      0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0, 0.0, -1.9400000013411045]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.4864000082015991, 0.4864000082015991, 0.0,\n",
            "      0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482,\n",
            "      0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509,\n",
            "      0.7202000021934509, 0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583, 0.7770000100135803,\n",
            "      0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845,\n",
            "      0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.029999999329447746, 0.029999999329447746]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.08037599990144371\n",
            "    policy_1: -0.07780199917033315\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15051849552772423\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306416200745006\n",
            "    mean_inference_ms: 3.960227731741272\n",
            "    mean_raw_obs_processing_ms: 6.8771886765413806\n",
            "time_since_restore: 5755.393399238586\n",
            "time_this_iter_s: 7.124914884567261\n",
            "time_total_s: 5755.393399238586\n",
            "timers:\n",
            "  learn_throughput: 259.524\n",
            "  learn_time_ms: 986.42\n",
            "  synch_weights_time_ms: 6.493\n",
            "  training_iteration_time_ms: 5778.646\n",
            "timestamp: 1677558514\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 255232\n",
            "training_iteration: 997\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1021952\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1021952\n",
            "  num_agent_steps_trained: 1021952\n",
            "  num_env_steps_sampled: 255488\n",
            "  num_env_steps_trained: 255488\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-41\n",
            "done: false\n",
            "episode_len_mean: 868.25\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.31635599814355375\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 312\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.913212162256241\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.0507365517318248\n",
            "        kl: 0.017166031085111853\n",
            "        policy_loss: -0.10965672866441309\n",
            "        total_loss: -0.12020756206475199\n",
            "        vf_explained_var: 0.4755144387483597\n",
            "        vf_loss: 0.0008644758134323637\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39900.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.7350375831127165\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.2623663514852523\n",
            "        kl: 0.016739352496539794\n",
            "        policy_loss: -0.11350057544186712\n",
            "        total_loss: -0.11937616630457341\n",
            "        vf_explained_var: 0.7331764176487923\n",
            "        vf_loss: 6.394257474084952e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39900.5\n",
            "  num_agent_steps_sampled: 1021952\n",
            "  num_agent_steps_trained: 1021952\n",
            "  num_env_steps_sampled: 255488\n",
            "  num_env_steps_trained: 255488\n",
            "iterations_since_restore: 998\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1021952\n",
            "num_agent_steps_trained: 1021952\n",
            "num_env_steps_sampled: 255488\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 255488\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 67.17777777777778\n",
            "  ram_util_percent: 37.81111111111111\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.08037599990144371\n",
            "  policy_1: -0.07780199917033315\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15051849552772423\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306416200745006\n",
            "  mean_inference_ms: 3.960227731741272\n",
            "  mean_raw_obs_processing_ms: 6.8771886765413806\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.31635599814355375\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618, 359, 751,\n",
            "      1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 323, 1000,\n",
            "      660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899, 48, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000, 1000, 530,\n",
            "      1000, 1000, 1000, 1000, 1000, 970]\n",
            "    episode_reward: [0.0, -1.0271999835968018, 0.0, -0.618399977684021, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035, 0.0,\n",
            "      -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0, -0.13399994373321533,\n",
            "      0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8628000020980835,\n",
            "      -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673, 0.0,\n",
            "      0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0, 0.0, -1.9400000013411045]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.4864000082015991, 0.4864000082015991, 0.0,\n",
            "      0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482,\n",
            "      0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509,\n",
            "      0.7202000021934509, 0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583, 0.7770000100135803,\n",
            "      0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845,\n",
            "      0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.029999999329447746, 0.029999999329447746]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.08037599990144371\n",
            "    policy_1: -0.07780199917033315\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15051849552772423\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306416200745006\n",
            "    mean_inference_ms: 3.960227731741272\n",
            "    mean_raw_obs_processing_ms: 6.8771886765413806\n",
            "time_since_restore: 5761.517693519592\n",
            "time_this_iter_s: 6.124294281005859\n",
            "time_total_s: 5761.517693519592\n",
            "timers:\n",
            "  learn_throughput: 259.735\n",
            "  learn_time_ms: 985.62\n",
            "  synch_weights_time_ms: 6.426\n",
            "  training_iteration_time_ms: 5800.613\n",
            "timestamp: 1677558521\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 255488\n",
            "training_iteration: 998\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1022976\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1022976\n",
            "  num_agent_steps_trained: 1022976\n",
            "  num_env_steps_sampled: 255744\n",
            "  num_env_steps_trained: 255744\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-47\n",
            "done: false\n",
            "episode_len_mean: 868.25\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.31635599814355375\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 312\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.8569959878921507\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.2277248904109002\n",
            "        kl: 0.018547767741660247\n",
            "        policy_loss: -0.10297963749617338\n",
            "        total_loss: -0.111472727637738\n",
            "        vf_explained_var: 0.6908441990613937\n",
            "        vf_loss: 0.00023721718898741528\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39940.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.5318084359169006\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.0958270981907845\n",
            "        kl: 0.016314745599266267\n",
            "        policy_loss: -0.11067089438438416\n",
            "        total_loss: -0.11505891932174564\n",
            "        vf_explained_var: 0.7702899470925331\n",
            "        vf_loss: 3.939904712524367e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39940.5\n",
            "  num_agent_steps_sampled: 1022976\n",
            "  num_agent_steps_trained: 1022976\n",
            "  num_env_steps_sampled: 255744\n",
            "  num_env_steps_trained: 255744\n",
            "iterations_since_restore: 999\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1022976\n",
            "num_agent_steps_trained: 1022976\n",
            "num_env_steps_sampled: 255744\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 255744\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 0.0\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.08037599990144371\n",
            "  policy_1: -0.07780199917033315\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15051849552772423\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306416200745006\n",
            "  mean_inference_ms: 3.960227731741272\n",
            "  mean_raw_obs_processing_ms: 6.8771886765413806\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.31635599814355375\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618, 359, 751,\n",
            "      1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 323, 1000,\n",
            "      660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899, 48, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000, 1000, 530,\n",
            "      1000, 1000, 1000, 1000, 1000, 970]\n",
            "    episode_reward: [0.0, -1.0271999835968018, 0.0, -0.618399977684021, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035, 0.0,\n",
            "      -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0, -0.13399994373321533,\n",
            "      0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8628000020980835,\n",
            "      -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673, 0.0,\n",
            "      0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0, 0.0, -1.9400000013411045]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.4864000082015991, 0.4864000082015991, 0.0,\n",
            "      0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482,\n",
            "      0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509,\n",
            "      0.7202000021934509, 0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583, 0.7770000100135803,\n",
            "      0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845,\n",
            "      0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.029999999329447746, 0.029999999329447746]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.08037599990144371\n",
            "    policy_1: -0.07780199917033315\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15051849552772423\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306416200745006\n",
            "    mean_inference_ms: 3.960227731741272\n",
            "    mean_raw_obs_processing_ms: 6.8771886765413806\n",
            "time_since_restore: 5768.229615449905\n",
            "time_this_iter_s: 6.71192193031311\n",
            "time_total_s: 5768.229615449905\n",
            "timers:\n",
            "  learn_throughput: 249.193\n",
            "  learn_time_ms: 1027.318\n",
            "  synch_weights_time_ms: 6.603\n",
            "  training_iteration_time_ms: 5961.6\n",
            "timestamp: 1677558527\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 255744\n",
            "training_iteration: 999\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n",
            "agent_timesteps_total: 1024000\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1024000\n",
            "  num_agent_steps_trained: 1024000\n",
            "  num_env_steps_sampled: 256000\n",
            "  num_env_steps_trained: 256000\n",
            "custom_metrics: {}\n",
            "date: 2023-02-28_04-28-53\n",
            "done: false\n",
            "episode_len_mean: 868.25\n",
            "episode_media: {}\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: -0.31635599814355375\n",
            "episode_reward_min: -1.9635999985039234\n",
            "episodes_this_iter: 0\n",
            "episodes_total: 312\n",
            "experiment_id: ce26487f99664e618dacf316bec87a1c\n",
            "hostname: 585f6ad2613d\n",
            "info:\n",
            "  learner:\n",
            "    policy_0:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.082440341822803\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.9425296068191527\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.1432886637747288\n",
            "        kl: 0.01746594114208345\n",
            "        policy_loss: -0.1121147175785154\n",
            "        total_loss: -0.12263414687477052\n",
            "        vf_explained_var: 0.6602222561836243\n",
            "        vf_loss: 0.0002838428121322067\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39980.5\n",
            "    policy_1:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 19.5\n",
            "      learner_stats:\n",
            "        allreduce_latency: 0.0\n",
            "        cur_kl_coeff: 1.282892256975174\n",
            "        cur_lr: 0.001\n",
            "        entropy: 2.740275192260742\n",
            "        entropy_coeff: 0.01\n",
            "        grad_gnorm: 1.4369840562343597\n",
            "        kl: 0.016847412031189758\n",
            "        policy_loss: -0.11117582535371184\n",
            "        total_loss: -0.1169651614734903\n",
            "        vf_explained_var: 0.6987189948558807\n",
            "        vf_loss: 4.604473582503488e-06\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 128.0\n",
            "      num_grad_updates_lifetime: 39980.5\n",
            "  num_agent_steps_sampled: 1024000\n",
            "  num_agent_steps_trained: 1024000\n",
            "  num_env_steps_sampled: 256000\n",
            "  num_env_steps_trained: 256000\n",
            "iterations_since_restore: 1000\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1024000\n",
            "num_agent_steps_trained: 1024000\n",
            "num_env_steps_sampled: 256000\n",
            "num_env_steps_sampled_this_iter: 256\n",
            "num_env_steps_trained: 256000\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 1\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 27.425\n",
            "  ram_util_percent: 37.8\n",
            "pid: 10094\n",
            "policy_reward_max:\n",
            "  policy_0: 0.9517999887466431\n",
            "  policy_1: 0.7770000100135803\n",
            "policy_reward_mean:\n",
            "  policy_0: -0.08037599990144371\n",
            "  policy_1: -0.07780199917033315\n",
            "policy_reward_min:\n",
            "  policy_0: -1.0\n",
            "  policy_1: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.15051849552772423\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 4.306416200745006\n",
            "  mean_inference_ms: 3.960227731741272\n",
            "  mean_raw_obs_processing_ms: 6.8771886765413806\n",
            "sampler_results:\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 868.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: -0.31635599814355375\n",
            "  episode_reward_min: -1.9635999985039234\n",
            "  episodes_this_iter: 0\n",
            "  hist_stats:\n",
            "    episode_lengths: [1000, 514, 1000, 309, 1000, 1000, 1000, 1000, 618, 359, 751,\n",
            "      1000, 175, 1000, 280, 778, 1000, 67, 1000, 107, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 432, 223, 1000, 1000, 1000, 1000, 1000, 711, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 802, 1000, 361, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 323, 1000,\n",
            "      660, 1000, 1000, 307, 1000, 1000, 961, 1000, 1000, 962, 899, 48, 1000, 1000,\n",
            "      1000, 1000, 1000, 1000, 1000, 655, 974, 1000, 1000, 1000, 1000, 1000, 1000,\n",
            "      1000, 1000, 1000, 448, 619, 1000, 1000, 1000, 982, 1000, 1000, 1000, 1000, 530,\n",
            "      1000, 1000, 1000, 1000, 1000, 970]\n",
            "    episode_reward: [0.0, -1.0271999835968018, 0.0, -0.618399977684021, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.2364000082015991, -0.7175999879837036, -1.5024000108242035, 0.0,\n",
            "      -0.3492000102996826, 0.0, -0.5595999956130981, -1.5552000105381012, 0.0, -0.13399994373321533,\n",
            "      0.0, -0.2136000394821167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8628000020980835,\n",
            "      -0.44599997997283936, 0.0, 0.0, 0.0, 0.0, 0.0, -1.420799970626831, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.6035999953746796, 0.0, -0.7208000421524048, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -0.6452000141143799, 0.0, -1.3191999793052673, 0.0,\n",
            "      0.0, -0.6131999492645264, 0.0, 0.0, -1.9215999990701675, 0.0, 0.0, -1.9231999963521957,\n",
            "      -1.7971999943256378, -0.09640002250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.3087999820709229, -1.9472000002861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -0.8955999612808228, -1.237999975681305, 0.0, 0.0, 0.0, -1.9635999985039234,\n",
            "      0.0, 0.0, 0.0, 0.0, -1.0587999820709229, 0.0, 0.0, 0.0, 0.0, 0.0, -1.9400000013411045]\n",
            "    policy_policy_0_reward: [0.0, 0.0, 0.4864000082015991, 0.4864000082015991, 0.0,\n",
            "      0.0, 0.6908000111579895, 0.6908000111579895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.38179999589920044, 0.38179999589920044, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.8253999948501587, 0.8253999948501587, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0,\n",
            "      0.0, 0.0, 0.9330000281333923, 0.9330000281333923, 0.0, 0.0, 0.8931999802589417,\n",
            "      0.8931999802589417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.19820000231266022, 0.19820000231266022, 0.0, 0.0, 0.6395999789237976, 0.6395999789237976,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.10140000283718109, 0.10140000283718109,\n",
            "      0.9517999887466431, 0.9517999887466431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3456000089645386, 0.3456000089645386, 0.026399999856948853,\n",
            "      0.026399999856948853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.018200000748038292, 0.018200000748038292, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.4706000089645386, 0.4706000089645386, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0]\n",
            "    policy_policy_1_reward: [0.0, 0.0, -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.6412000060081482, 0.6412000060081482,\n",
            "      0.24879999458789825, 0.24879999458789825, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.7202000021934509,\n",
            "      0.7202000021934509, 0.2223999947309494, 0.2223999947309494, 0.0, 0.0, -1.0,\n",
            "      -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.5685999989509583, 0.5685999989509583, 0.7770000100135803,\n",
            "      0.7770000100135803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896000146865845,\n",
            "      0.2896000146865845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.6773999929428101, 0.6773999929428101, 0.0, 0.0, 0.34040001034736633,\n",
            "      0.34040001034736633, 0.0, 0.0, 0.0, 0.0, 0.6934000253677368, 0.6934000253677368,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.03920000046491623, 0.03920000046491623, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.03840000182390213, 0.03840000182390213, -1.0, -1.0, -1.0, -1.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0,\n",
            "      -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, 0.5522000193595886, 0.5522000193595886, 0.38100001215934753,\n",
            "      0.38100001215934753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
            "      0.0, 0.0, 0.029999999329447746, 0.029999999329447746]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max:\n",
            "    policy_0: 0.9517999887466431\n",
            "    policy_1: 0.7770000100135803\n",
            "  policy_reward_mean:\n",
            "    policy_0: -0.08037599990144371\n",
            "    policy_1: -0.07780199917033315\n",
            "  policy_reward_min:\n",
            "    policy_0: -1.0\n",
            "    policy_1: -1.0\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.15051849552772423\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 4.306416200745006\n",
            "    mean_inference_ms: 3.960227731741272\n",
            "    mean_raw_obs_processing_ms: 6.8771886765413806\n",
            "time_since_restore: 5773.350984573364\n",
            "time_this_iter_s: 5.121369123458862\n",
            "time_total_s: 5773.350984573364\n",
            "timers:\n",
            "  learn_throughput: 257.364\n",
            "  learn_time_ms: 994.701\n",
            "  synch_weights_time_ms: 6.393\n",
            "  training_iteration_time_ms: 5882.808\n",
            "timestamp: 1677558533\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 256000\n",
            "training_iteration: 1000\n",
            "trial_id: default\n",
            "warmup_time: 23.96312165260315\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r '/content/drive/MyDrive/results_soccer5.zip' '/content/results/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNJ6B25T19z2",
        "outputId": "615631c4-70af-4e14-e53f-acf6c994aaf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/results/ (stored 0%)\n",
            "  adding: content/results/ppo/ (stored 0%)\n",
            "  adding: content/results/ppo/rllib_checkpoint.json (deflated 19%)\n",
            "  adding: content/results/ppo/policies/ (stored 0%)\n",
            "  adding: content/results/ppo/policies/policy_1/ (stored 0%)\n",
            "  adding: content/results/ppo/policies/policy_1/rllib_checkpoint.json (deflated 19%)\n",
            "  adding: content/results/ppo/policies/policy_1/policy_state.pkl (deflated 7%)\n",
            "  adding: content/results/ppo/policies/policy_0/ (stored 0%)\n",
            "  adding: content/results/ppo/policies/policy_0/rllib_checkpoint.json (deflated 19%)\n",
            "  adding: content/results/ppo/policies/policy_0/policy_state.pkl (deflated 7%)\n",
            "  adding: content/results/ppo/algorithm_state.pkl (deflated 69%)\n"
          ]
        }
      ]
    }
  ]
}